# âš¡ Quick Fix: Slow Translation (12-24s â†’ 2-4s)

## ğŸ”´ Problem
Translation taking 12-24 seconds because PyTorch is **CPU-only** (not using your RTX 3050)

## âœ… Solution (3 steps, 5 minutes)

### Step 1: Check Current State
```bash
cd backend
python check_gpu.py
```

**If shows "CUDA Available: False" â†’ Proceed to Step 2**

### Step 2: Install GPU Support
```bash
powershell -ExecutionPolicy Bypass -File install_gpu_support.ps1
```

**Wait 2-3 minutes for PyTorch CUDA to install...**

### Step 3: Restart Backend
```bash
python main.py
```

**Look for these lines:**
```
ğŸš€ Preloading AI models...
INFO - Whisper model loaded on cuda  âœ…
INFO - TTS model loaded successfully on cuda  âœ…
âœ… All AI models preloaded and ready!
```

## ğŸ¯ Expected Result

**Before:**
- Translation: 12-24 seconds âŒ
- First request: Very slow
- GPU usage: 0%

**After:**
- Translation: 2-4 seconds âœ… **5-10x faster!**
- First request: Instant (preloaded)
- GPU usage: 40-80%

## ğŸ§ª Test It

1. Start backend: `python main.py`
2. Open two browser windows
3. Connect with different languages
4. Speak â†’ Should hear translation in **2-4 seconds**!

## ğŸ“š More Details

See `GPU_OPTIMIZATION_GUIDE.md` for full documentation

---

**TL;DR:** Run `install_gpu_support.ps1` in backend folder â†’ Restart backend â†’ 5-10x faster!
